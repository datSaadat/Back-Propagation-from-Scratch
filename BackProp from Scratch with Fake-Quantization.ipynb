{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFpsw1zg23vX",
        "outputId": "47e91c3a-c85f-44f7-ff2d-0901827ff0fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import scipy as sp\n",
        "import copy\n",
        "import pickle\n",
        "\n",
        "\n",
        "from keras.datasets import mnist\n",
        "(train_data, train_labels), (test_data, test_labels) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "q2DBNNCRH_wl"
      },
      "outputs": [],
      "source": [
        "def hot_encoding(array):\n",
        "    hot_encoded_out = []\n",
        "    x = list(range(min(array),max(array)+1))\n",
        "\n",
        "    for i in range (0,len(array)):\n",
        "        hot_encoded_out.append(((x == array[i]).astype(int)))\n",
        "\n",
        "    return np.array(hot_encoded_out).T\n",
        "\n",
        "def fake_quantize(tensor, bit_width, scale=None, min_val=None, max_val=None):\n",
        "    \n",
        "    if min_val is None:\n",
        "        min_val = np.min(tensor)\n",
        "    if max_val is None:\n",
        "        max_val = np.max(tensor)\n",
        "\n",
        "    # Calculate the quantization range\n",
        "    qmin = 0\n",
        "    qmax = 2**bit_width - 1\n",
        "\n",
        "    # Calculate the scale and zero point\n",
        "    if scale is None:\n",
        "        scale = (max_val - min_val) / (qmax - qmin)\n",
        "    # print(scale)\n",
        "    zero_point = qmin - min_val / scale\n",
        "\n",
        "    # Fake quantization: Quantize and dequantize\n",
        "    quantized_tensor = np.round(tensor / scale + zero_point)\n",
        "    quantized_tensor = np.clip(quantized_tensor, qmin, qmax)\n",
        "\n",
        "    # Dequantize back to the floating-point representation\n",
        "    dequantized_tensor = (quantized_tensor - zero_point) * scale\n",
        "\n",
        "    return dequantized_tensor, scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B55-Hy_KDMyV",
        "outputId": "b944a6de-fa6c-424c-8b1f-c2839a5985a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(784, 60000)\n",
            "(10, 60000)\n",
            "(784, 10000)\n",
            "(10, 10000)\n"
          ]
        }
      ],
      "source": [
        "#Reshape the  data examples\n",
        "train_x_flatten = train_data.reshape(train_data.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
        "test_x_flatten = test_data.reshape(test_data.shape[0], -1).T\n",
        "\n",
        "#Standardize data to have feature values between 0 and 1\n",
        "train_x = train_x_flatten/255\n",
        "test_x = test_x_flatten/255\n",
        "\n",
        "#Hot encoding the train and test samples\n",
        "train_y = hot_encoding(np.squeeze(train_labels))\n",
        "test_y = hot_encoding(np.squeeze(test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "suleMwK_ayGS"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def softmax(Z):\n",
        "    p = Z - np.max(Z)\n",
        "    A = np.exp(p) / (np.sum(np.exp(p), axis=0))\n",
        "    assert (np.shape(A) == np.shape(Z))\n",
        "    return A\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "  return 1 - x**2\n",
        "\n",
        "def mse(x, y):\n",
        "  return np.sum((x-y)**2)/(x.shape[1])\n",
        "\n",
        "def dot_sigmoid(W, b, h_prev):\n",
        "  return sigmoid(np.dot(W, h_prev) + b)\n",
        "\n",
        "def dot_softmax(W, b, h_prev):\n",
        "  return softmax(np.dot(W, h_prev) + b)\n",
        "\n",
        "def cost_bincentropy(h, y):\n",
        "  return -1* np.sum((y*np.log(h)+(1-y)*(1-np.log(h))))/h.shape[1]\n",
        "\n",
        "def predict(p):\n",
        "  return np.round(p)\n",
        "\n",
        "def cal_accuracy(pred, truth):\n",
        "  corr = 0\n",
        "  for i in range(0,pred.shape[1]):\n",
        "    if np.sum(np.abs((pred[:,i]-truth[:,i])))==0:\n",
        "      corr+=1\n",
        "  return corr/pred.shape[1]*100\n",
        "\n",
        "\n",
        "def one_hot_max(prediction_probs):\n",
        "    max_indices = np.argmax(prediction_probs, axis=0)\n",
        "    one_hot_result = np.zeros_like(prediction_probs)\n",
        "    one_hot_result[max_indices, np.arange(prediction_probs.shape[1])] = 1\n",
        "\n",
        "    return one_hot_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xD6XGnAv_qZU"
      },
      "outputs": [],
      "source": [
        "def initialize_param_he(Layers):\n",
        "  # np.random.seed(4)\n",
        "  param= {}\n",
        "  layers_count = len(Layers)\n",
        "\n",
        "  for i in range(layers_count - 1):\n",
        "    param[\"F\"+str(i+1)] = np.random.normal(0, np.sqrt(2.0 / (Layers[i+1]+Layers[i])), (Layers[i+1], Layers[i]))\n",
        "\n",
        "    param[\"f\"+str(i+1)] = np.zeros((Layers[i+1], 1))\n",
        "\n",
        "  return param\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "daF_u3IlP5a_"
      },
      "outputs": [],
      "source": [
        "def bp_grads_final(hp, hp_cap, h):     #hp_cap here target labels\n",
        "  pers = h.shape[1]\n",
        "  grad_Z_Last = hp - hp_cap                             # predictions - truth\n",
        "  grad_F = np.dot(grad_Z_Last, h.T)/pers\n",
        "  grad_f = np.sum(grad_Z_Last, axis=1, keepdims=True)/pers\n",
        "\n",
        "  return grad_F, grad_f, grad_Z_Last\n",
        "\n",
        "\n",
        "def bp_grads(W, hp, h, grad_Zpp, bit_width):     #hp is present, Zpp is p+1, h is previous\n",
        "  pers = h.shape[1]\n",
        "\n",
        "  part_grad_Zp, _ = fake_quantize(np.dot(W.T, grad_Zpp), bit_width) # quantize after dot product\n",
        "  grad_Zp = part_grad_Zp*sigmoid_derivative(hp)\n",
        "  q_grad_Zp_in_grads, _ = fake_quantize(grad_Zp, bit_width)\n",
        "  grad_F = np.dot(q_grad_Zp_in_grads, h.T)/pers\n",
        "  grad_f = np.sum(q_grad_Zp_in_grads, axis=1, keepdims=True)/pers\n",
        "\n",
        "  return grad_F, grad_f, grad_Zp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PNorXgpILiBP"
      },
      "outputs": [],
      "source": [
        "\n",
        "def model_forward(PARAM, input_data):\n",
        "  L = int(len(PARAM)/2)+1\n",
        "  h = input_data\n",
        "\n",
        "  for loop in range(1, L):\n",
        "    if loop < L-1:\n",
        "      h = dot_sigmoid(PARAM[\"F\"+str(loop)], PARAM[\"f\"+str(loop)], h)\n",
        "    \n",
        "    else:\n",
        "      h = dot_softmax(PARAM[\"F\"+str(loop)], PARAM[\"f\"+str(loop)], h)\n",
        "\n",
        "  return h\n",
        "\n",
        "\n",
        "def q_model_forward(PARAM, input_data, bit_width):\n",
        "  L = int(len(PARAM)/2)+1\n",
        "  h = input_data\n",
        "  h, _ = fake_quantize(h, bit_width)\n",
        "  # print(L)\n",
        "\n",
        "  for loop in range(1, L):\n",
        "    if loop < L-1:\n",
        "      q_F, _ = fake_quantize(PARAM[\"F\"+str(loop)], bit_width)\n",
        "      q_f, _ = fake_quantize(PARAM[\"f\"+str(loop)], bit_width)\n",
        "      h = dot_sigmoid(q_F, q_f, h)\n",
        "      h, _ = fake_quantize(h, bit_width)\n",
        "    \n",
        "    else:\n",
        "      q_F, _ = fake_quantize(PARAM[\"F\"+str(loop)], bit_width)\n",
        "      q_f, _ = fake_quantize(PARAM[\"f\"+str(loop)], bit_width)\n",
        "      h = dot_softmax(q_F, q_f, h)\n",
        "      h, _ = fake_quantize(h, bit_width)\n",
        "\n",
        "  return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PJdxzmsL24kP"
      },
      "outputs": [],
      "source": [
        "def back_propagation(input_data_main, target_output_main, hidden_layers, epochs, learning_rate, batches, bit_width):\n",
        "\n",
        "    cost_back = np.array([])\n",
        "    cost_for = np.array([])\n",
        "    train_accu = np.array([])\n",
        "    accu_old = -1\n",
        "    train_accu_train =np.array([])\n",
        "\n",
        "\n",
        "    Ns = target_output_main.shape[1]\n",
        "    per_batch = int(Ns/batches)\n",
        "\n",
        "    # np.random.seed(0) # for 0,1\n",
        "    # np.random.seed(4)\n",
        "\n",
        "    # Initialize weights and biases\n",
        "    n0 = input_data_main.shape[0]\n",
        "    n_L = target_output_main.shape[0]\n",
        "\n",
        "    L = len(hidden_layers)+2   # Number of hidden layers plus input and output layers\n",
        "    parameters = initialize_param_he([n0]+hidden_layers+[n_L])\n",
        "    q_param = {}\n",
        "    activations = {}\n",
        "    q_activations = {}\n",
        "    grad = {}\n",
        "\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "      random_new_indices = random.sample(range(0,Ns), Ns)\n",
        "\n",
        "      input_data_ = input_data_main[:,random_new_indices]\n",
        "      target_output_ = target_output_main[:,random_new_indices]\n",
        "\n",
        "      for i in range(0,  batches):\n",
        "        input_data = input_data_[:, i*per_batch:(i+1)*per_batch]\n",
        "        target_output = target_output_[:, i*per_batch:(i+1)*per_batch]\n",
        "\n",
        "        #quantize input\n",
        "        activations[\"h0\"] = input_data\n",
        "        q_activations[\"h0\"], _ = fake_quantize(input_data, bit_width)\n",
        "\n",
        "        #quantize weights and biases\n",
        "        for loop in range(1, L):\n",
        "          q_param[\"F\"+str(loop)], scale_F = fake_quantize(parameters[\"F\"+str(loop)], bit_width)\n",
        "          q_param[\"f\"+str(loop)], _ = fake_quantize(parameters[\"f\"+str(loop)], bit_width, scale=scale_F)\n",
        "\n",
        "\n",
        "        # # Forward pass\n",
        "        for loop in range(1, L):\n",
        "          if loop < L-1:\n",
        "            activations[\"h\"+str(loop)] = dot_sigmoid(q_param[\"F\"+str(loop)], q_param[\"f\"+str(loop)], q_activations[\"h\"+str(loop-1)])\n",
        "            q_activations[\"h\"+str(loop)], _ = fake_quantize(activations[\"h\"+str(loop)], bit_width)\n",
        "          else:\n",
        "            activations[\"h\"+str(loop)] = dot_softmax(q_param[\"F\"+str(loop)], q_param[\"f\"+str(loop)], q_activations[\"h\"+str(loop-1)])\n",
        "            q_activations[\"h\"+str(loop)], _ = fake_quantize(activations[\"h\"+str(loop)], bit_width)\n",
        "\n",
        "\n",
        "        # Compute cost\n",
        "        cost_for = np.append(cost_for, mse(activations[\"h\"+str(L-1)], target_output))\n",
        "\n",
        "        # Compute grads for forward weights and biases\n",
        "        for loop in reversed(range(1, L)):\n",
        "          if loop == L-1:\n",
        "            grad[\"F\"+str(loop)], grad[\"f\"+str(loop)], grad_Z = bp_grads_final(q_activations[\"h\"+str(loop)], target_output, q_activations[\"h\"+str(loop-1)])\n",
        "            q_grad_Z, _ = fake_quantize(grad_Z, bit_width)\n",
        "          else:\n",
        "            grad[\"F\"+str(loop)], grad[\"f\"+str(loop)], grad_Z = bp_grads(q_param[\"F\"+str(loop+1)], q_activations[\"h\"+str(loop)],\n",
        "                                                                           q_activations[\"h\"+str(loop-1)], q_grad_Z, bit_width)\n",
        "            q_grad_Z, _ = fake_quantize(grad_Z, bit_width)\n",
        "            \n",
        "\n",
        "\n",
        "        #update parameters\n",
        "        for loop in range(1, L):\n",
        "          parameters[\"F\"+str(loop)] -= grad[\"F\"+str(loop)] * learning_rate\n",
        "          parameters[\"f\"+str(loop)] -= grad[\"f\"+str(loop)] * learning_rate\n",
        " \n",
        "\n",
        "      #calculate training accuracy\n",
        "      final_layer = q_model_forward(parameters, input_data_main, bit_width)\n",
        "      prediction = one_hot_max(final_layer)\n",
        "      accu_new_train = cal_accuracy(prediction, target_output_main)\n",
        "      train_accu_train = np.append(train_accu_train, accu_new_train)\n",
        "\n",
        "\n",
        "      #calculate validation accuracy\n",
        "      final_layer = q_model_forward(parameters, test_x[:,0:4000], bit_width)\n",
        "      prediction = one_hot_max(final_layer)\n",
        "      accu_new = cal_accuracy(prediction, test_y[:,0:4000])\n",
        "      train_accu = np.append(train_accu, accu_new)\n",
        "\n",
        "\n",
        "\n",
        "      if accu_new>=accu_old or accu_old==-1:\n",
        "        parameters_final = copy.deepcopy(parameters)\n",
        "        accu_old = accu_new\n",
        "\n",
        "        print(f'Iteration {epoch} ---------')\n",
        "        print(f'Training Accuracy: {accu_new_train}')\n",
        "        print(f'Validation Accuracy: {accu_old}')\n",
        "\n",
        "\n",
        "    plt.plot(np.squeeze(cost_for))\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(np.squeeze(train_accu))\n",
        "    plt.plot(np.squeeze(train_accu_train))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    return parameters_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PpRh0OByDMya",
        "outputId": "033f7d9e-8142-4e18-c819-da228a8c05e3"
      },
      "outputs": [],
      "source": [
        "# Training the network\n",
        "\n",
        "learning_rate = 0.1     #Learning rate\n",
        "batch = 128             #Batch size\n",
        "bit_width = 4           #Bit width\n",
        "epochs = 100            #Number of epochs\n",
        "hidden_layers = [128, 64]    #Hidden layers\n",
        "\n",
        "\n",
        "parameters_final = back_propagation(train_x,\n",
        "                                    train_y,\n",
        "                                    hidden_layers,\n",
        "                                    epochs,\n",
        "                                    learning_rate,\n",
        "                                    batch,\n",
        "                                    bit_width)\n",
        "#testing network\n",
        "input_test_data= test_x\n",
        "final_layer_output = q_model_forward(parameters_final, input_test_data, bit_width)\n",
        "prediction = one_hot_max(final_layer_output)\n",
        "print(\"Test Accuracy:\")\n",
        "print(cal_accuracy(prediction, test_y))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
