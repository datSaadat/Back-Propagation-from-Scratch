# -*- coding: utf-8 -*-
"""BP_Scratch_numpy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NuHna2fVZ9VfSQ5p011hOHyF_Lmk8-d4
"""

import numpy as np
import matplotlib.pyplot as plt
import random
import scipy as sp
import copy
import keras

from keras.datasets import mnist
(train_data, train_labels), (test_data, test_labels) = mnist.load_data()

def hot_encoding(array):

    hot_encoded_out = []
    x = list(range(min(array),max(array)+1))

    for i in range (0,len(array)):
        hot_encoded_out.append(((x == array[i]).astype(int)))

    return np.array(hot_encoded_out).T

# State independent, alpha 0.00
"""Reshape the  data examples"""
train_x_flatten = train_data.reshape(train_data.shape[0], -1).T   # The "-1" makes reshape flatten the remaining dimensions
test_x_flatten = test_data.reshape(test_data.shape[0], -1).T

""" Standardize data to have feature values between 0 and 1"""
train_x = train_x_flatten/255
test_x = test_x_flatten/255

# train_x = train_data
# test_x = test_data

"""Hot encoding the train and test samples"""
train_y = hot_encoding(np.squeeze(train_labels))
test_y = hot_encoding(np.squeeze(test_labels))



print(np.shape(train_x))
print(np.shape(train_y))
print(np.shape(test_x))
print(np.shape(test_y))


# train_x = np.asarray(train_x)
# train_y = np.asarray(train_y)
# test_x = np.asarray(test_x)
# test_y = np.asarray(test_y)

def sigmoid(x):
  # x = ADC(x)
  return 1 / (1 + np.exp(-x))

def softmax(Z):
    # Z = ADC(Z)
    # print(np.max(Z))
    p = Z - np.max(Z)
    A = np.exp(p) / (np.sum(np.exp(p), axis=0))
    # print(np.max(Z))
    # print(np.min(Z))
    assert (np.shape(A) == np.shape(Z))
    return A

def sigmoid_derivative(x):
  return x * (1 - x)

def mse(x, y):
  return np.sum((x-y)**2)/(x.shape[1])

def dot_sigmoid(W, b, h_prev):
  # W += add_gaussian_noise(W, std = 0.01)
  return sigmoid(np.dot(W, h_prev) + b)

def dot_softmax(W, b, h_prev):
  # W += add_gaussian_noise(W, std = 0.01)
  return softmax(np.dot(W, h_prev) + b)

def cost_bincentropy(h, y):
  return -1* np.sum((y*np.log(h)+(1-y)*(1-np.log(h))))/h.shape[1]

def predict(p):
  return np.round(p)

def cal_accuracy(pred, truth):
  corr = 0
  for i in range(0,pred.shape[1]):
    # print(pred[:,i])
    # print(truth[:,i])
    if np.sum(np.abs((pred[:,i]-truth[:,i])))==0:
      corr+=1
  return corr/pred.shape[1]*100


def one_hot_max(prediction_probs):
    max_indices = np.argmax(prediction_probs, axis=0)
    one_hot_result = np.zeros_like(prediction_probs)
    one_hot_result[max_indices, np.arange(prediction_probs.shape[1])] = 1

    return one_hot_result

def initialize_param(Layers):
  # np.random.seed(0)
  param= {}
  layers_count = len(Layers)

  for i in range(layers_count - 1):
    param["F"+str(i+1)] = np.random.normal(0, 1, (Layers[i+1], Layers[i]))
    param["f"+str(i+1)] = np.zeros((Layers[i+1], 1))

    if i > 0 and i < (layers_count-2):
      param["G"+str(i+1)] = np.random.normal(0, 1, (Layers[i], Layers[i+1]))
      param["g"+str(i+1)] = np.zeros((Layers[i], 1))

  return param

def initialize_param_he(Layers):
  # np.random.seed(4)
  param= {}
  layers_count = len(Layers)

  for i in range(layers_count - 1):
    param["F"+str(i+1)] = np.random.normal(0, np.sqrt(2.0 / Layers[i]), (Layers[i+1], Layers[i]))
    #param["F"+str(i+1)] = np.random.normal(5e-6, 1e-6, (Layers[i+1], Layers[i]))

    param["f"+str(i+1)] = np.zeros((Layers[i+1], 1))

    if i > 0 and i < (layers_count-2):
      param["G"+str(i+1)] = np.random.normal(0, np.sqrt(2.0 / Layers[i+1]), (Layers[i], Layers[i+1]))
      #param["G"+str(i+1)] = np.random.normal(5e-6, 1e-6, (Layers[i], Layers[i+1]))

      param["g"+str(i+1)] = np.zeros((Layers[i], 1))

  return param

def bp_grads_final(hp, hp_cap, h):     #hp_cap here target labels
  pers = h.shape[1]
  grad_Z_Last = hp - hp_cap                             # predictions - truth
  grad_F = np.dot(grad_Z_Last, h.T)/pers
  grad_f = np.sum(grad_Z_Last, axis=1, keepdims=True)/pers

  return grad_F, grad_f, grad_Z_Last


def bp_grads(W, hp, h, grad_Zpp):     #hp is present, Zpp is p+1, h is previous
  pers = h.shape[1]

  grad_Zp = np.dot(W.T, grad_Zpp)*sigmoid_derivative(hp)
  grad_F = np.dot(grad_Zp, h.T)/pers
  grad_f = np.sum(grad_Zp, axis=1, keepdims=True)/pers

  return grad_F, grad_f, grad_Zp

def ADC(matrix):
  # Example matrix (replace this with your actual matrix)


  # Define bin edges (16 states from -16 to 16)
  bin_edges = np.linspace(-16, 16, 17)

  # Discretize the matrix elements
  discretized_matrix = np.digitize(matrix, bins=bin_edges) - 9  # Subtract 1 to convert bin index to 0-based index

  # Clip values to ensure they are within the desired range
  discretized_matrix = np.clip(discretized_matrix, -7, 7)
  float_matrix = discretized_matrix.astype(float)

  return float_matrix

def model_forward(PARAM, input_data):
  F1 = PARAM["F1"]
  F2 = PARAM["F2"]
  F3 = PARAM["F3"]
  f1 = PARAM["f1"]
  f2 = PARAM["f2"]
  f3 = PARAM["f3"]

  # G2 = PARAM["G2"]
  # g2 = PARAM["g2"]
  return dot_softmax(F3, f3, dot_sigmoid(F2, f2, dot_sigmoid(F1, f1, input_data)))

def back_propagation(input_data_main, target_output_main, epochs, learning_rate, batches, alpha, state_depend):

    cost_back = np.array([])
    cost_for = np.array([])
    train_accu = np.array([])
    accu_old = -1
    train_accu_train =np.array([])


    Ns = target_output_main.shape[1]
    per_batch = int(Ns/batches)

    # np.random.seed(0) # for 0,1
    # np.random.seed(4)
    # Initialize weights and biases
    n0 = input_data_main.shape[0]
    n1 = 128
    n2 = 64
    n3 = target_output_main.shape[0]


    parameters = initialize_param([n0, n1, n2, n3])


    F1 = parameters["F1"]
    F2 = parameters["F2"]
    F3 = parameters["F3"]
    f1 = parameters["f1"]
    f2 = parameters["f2"]
    f3 = parameters["f3"]

    # Training loop
    for epoch in range(epochs):

      random_new_indices = random.sample(range(0,Ns), Ns)

      input_data_ = input_data_main[:,random_new_indices]
      target_output_ = target_output_main[:,random_new_indices]

      for i in range(0,  batches):
        input_data = input_data_[:, i*per_batch:(i+1)*per_batch]
        target_output = target_output_[:, i*per_batch:(i+1)*per_batch]
        # Forward pass
        h1 = dot_sigmoid(F1, f1, input_data)
        h2 = dot_sigmoid(F2, f2, h1)
        h3 = dot_softmax(F3, f3, h2)

        cost_mse = mse(h3, target_output)
        cost_new = cost_mse
        cost_for = np.append(cost_for, cost_mse)


        # Compute grads for forward weights

        grad_F3, grad_f3, grad_Z3 = bp_grads_final(h3, target_output, h2)

        assert (grad_F3.shape == (n3, n2))
        assert (grad_f3.shape == (n3, 1))

        grad_F2, grad_f2, grad_Z2 = bp_grads(F3, h2, h1, grad_Z3)

        assert (grad_F2.shape == (n2, n1))
        assert (grad_f2.shape == (n2, 1))

        grad_F1, grad_f1, grad_Z1 = bp_grads(F2, h1, input_data, grad_Z2)

        assert (grad_F1.shape == (n1, n0))
        assert (grad_f1.shape == (n1, 1))


        F3 -= grad_F3 * learning_rate
        f3 -= grad_f3 * learning_rate
        F2 -= grad_F2 * learning_rate
        f2 -= grad_f2 * learning_rate
        F1 -= grad_F1 * learning_rate
        f1 -= grad_f1 * learning_rate

      #calculate training accuracy
      final_layer = model_forward(parameters, input_data_main)
      prediction = one_hot_max(final_layer)
      accu_new_train = cal_accuracy(prediction, target_output_main)
      train_accu_train = np.append(train_accu_train, accu_new_train)
      # print(f'Accuracy New: {accu_new}')


      #calculate validation accuracy
      final_layer = model_forward(parameters, test_x[:,0:4000])
      prediction = one_hot_max(final_layer)
      accu_new = cal_accuracy(prediction, test_y[:,0:4000])
      train_accu = np.append(train_accu, accu_new)
      # print(f'Accuracy New: {accu_new}')


      if accu_new>=accu_old or accu_old==-1:
        parameters_final = copy.deepcopy(parameters)

        accu_old = accu_new

        print(f'Iteration {epoch} ---------')
        print(f'Training Accuracy: {accu_new_train}')
        print(f'Validation Accuracy: {accu_old}')


    plt.plot(np.squeeze(cost_for))
    plt.show()

    plt.plot(np.squeeze(train_accu))
    plt.plot(np.squeeze(train_accu_train))
    plt.show()


    return parameters_final

learning_rate = 1.2
batch = 128
state_depend = False
epochs = 200
alpha = 0

parameters_final = back_propagation(train_x,
                                    train_y,
                                    epochs,
                                    learning_rate,
                                    batch,
                                    alpha,
                                    state_depend )

#testing network
input_test_data= test_x
final_layer_output = model_forward(parameters_final, input_test_data)
prediction = one_hot_max(final_layer_output)
print("Accuracy:")
print(cal_accuracy(prediction, test_y))